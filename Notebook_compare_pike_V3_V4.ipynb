{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b127c58-0049-4c23-9217-8639254669c0",
   "metadata": {},
   "source": [
    "**V3_V4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4bd01fe-d6da-4b44-acec-8dcf8bac8ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from os import listdir\n",
    "from pandas import read_csv, DataFrame\n",
    "from tqdm import tqdm\n",
    "from subprocess import call\n",
    "from Bio.SeqIO import parse\n",
    "from skbio.stats.composition import clr \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec82cd14-3bc0-4ae0-8ce2-482378529645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaleido\n",
      "  Using cached kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\n",
      "Using cached kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
      "Installing collected packages: kaleido\n",
      "Successfully installed kaleido-0.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec33e85-0385-4a5d-9697-36b095d432c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fasta(output, mearged_pike_out, dbpath):\n",
    "    # Create fasta file \n",
    "    consensus = {}\n",
    "    cons_conter = 0\n",
    "    \n",
    "    with open(f'{output}/all_consensus.fasta', 'w') as opn_fasta:\n",
    "        for cons in mearged_pike_out.index:\n",
    "    \n",
    "            opn_fasta.write(f'>{cons_conter}\\n{cons}\\n')\n",
    "            consensus[cons_conter] = cons\n",
    "            cons_conter += 1\n",
    "    \n",
    "    return consensus\n",
    "    \n",
    "def run_blast(base, path):\n",
    "    \n",
    "    call(f'makeblastdb -in {base} -dbtype nucl', shell=True)\n",
    "    call(f'blastn -num_threads 60  -outfmt \"7 qseqid sseqid pident evalue qcovs bitscore\" -query {path}/all_consensus.fasta  -db {base} -out {path}/blast_results.txt', shell=True)\n",
    " #   pass\n",
    "def decode_tax(base) -> dict:\n",
    "    \n",
    "    # DB decoder \n",
    "    # Use db header format: Kingdom    Phylum    Class    Order    Family    Genus    Species\n",
    "    \n",
    "    base = parse(base, 'fasta')\n",
    "    taxonomy_linage = ['Kingdom', 'Phylum', 'Class', 'Order', 'Family', 'Genus', 'Species']\n",
    "    tax_decoder = {}\n",
    "    \n",
    "    for line in tqdm(base):\n",
    "        \n",
    "        tax_decoder[line.id] = {}\n",
    "        linage = line.description.split(';')\n",
    "        linage[0] = linage[0].split()[1]\n",
    "    \n",
    "        for i in range(len(taxonomy_linage)):\n",
    "            try:\n",
    "                if taxonomy_linage[i] != 'Species':\n",
    "                \n",
    "                    tax_decoder[line.id][taxonomy_linage[i]] = linage[i]\n",
    "                \n",
    "                else:\n",
    "                    #print(linage[i].split())\n",
    "                    tax_decoder[line.id][taxonomy_linage[i]] = ' '.join(linage[i].split()[:2])\n",
    "                  \n",
    "            except:\n",
    "                \n",
    "                tax_decoder[line.id][taxonomy_linage[i]] = 'NA'\n",
    "    \n",
    "    return tax_decoder\n",
    "\n",
    "def parse_blast(path, \n",
    "                base, \n",
    "                data_tax, \n",
    "                consensus, \n",
    "                identity_filter, \n",
    "                cov_lim, \n",
    "                evalue_filter):\n",
    "    \n",
    "    # parser of blast table\n",
    "    \n",
    "    blast_header = ['qseqid',\n",
    "                    'sseqid', \n",
    "                    'pident',\n",
    "                    'evalue',\n",
    "                    'qcovs', \n",
    "                    'bitscore']\n",
    "    \n",
    "    blasting_results = {}\n",
    "    opn_blast = read_csv(f'{path}/blast_results.txt', sep='\\t', comment='#', header=None, names=blast_header)\n",
    "    \n",
    "    for i in tqdm(opn_blast['qseqid'].unique()):\n",
    "        \n",
    "        blast_subset = opn_blast[opn_blast[\"qseqid\"] == i]\n",
    "        blast_subset = blast_subset[blast_subset['pident'] >= identity_filter]\n",
    "        blast_subset = blast_subset[blast_subset['evalue'] <= evalue_filter]\n",
    "        blast_subset = blast_subset[blast_subset['qcovs'] >= cov_lim]\n",
    "\n",
    "        blast_subset = blast_subset.sort_values(by='evalue')\n",
    "        blast_subset = blast_subset.sort_values(by='pident')[::-1]\n",
    "\n",
    "        if len(blast_subset['sseqid'].values) == 0:\n",
    "            continue\n",
    "            \n",
    "        subject = blast_subset['sseqid'].values[0]\n",
    "        blasting_results[consensus[i]] = data_tax[subject]\n",
    "        \n",
    "    blasting_results_df = DataFrame(blasting_results).T\n",
    "    \n",
    "    return blasting_results_df\n",
    "    \n",
    "def processing_data_tax(data_tax):\n",
    "\n",
    "    data_tax_df = DataFrame(data_tax).T.fillna(0)\n",
    "    # Add pseudocunt\n",
    "    # data_tax_df = data_tax_df + 1\n",
    "    data_tax_df = data_tax_df.assign(m=data_tax_df.mean(axis=1)).sort_values('m').drop('m', axis=1)[np.sort(data_tax_df.columns)]\n",
    "\n",
    "    return  data_tax_df\n",
    "    \n",
    "def get_taxonomy(data_tax, \n",
    "                 blasting_results_df, \n",
    "                 mearged_pike_out, \n",
    "                 tax_level='OTU'):\n",
    "    \n",
    "    data_tax = {}\n",
    "    avs = np.intersect1d(blasting_results_df.index, mearged_pike_out.index)\n",
    "    count = 1\n",
    "    OTU_decoder  = {'Seq': [], 'OTU_name' : []}\n",
    "    \n",
    "    for av in tqdm(avs):\n",
    "\n",
    "        if tax_level == 'OTU':\n",
    "        \n",
    "            tax = f'OTU_{count}_{blasting_results_df[\"Species\"][av]}'\n",
    "        else:    \n",
    "            tax = blasting_results_df[tax_level][av]\n",
    "        count += 1\n",
    "        OTU_decoder['Seq'].append(av)\n",
    "        OTU_decoder['OTU_name'].append(tax)\n",
    "        if tax == 'nan':\n",
    "            \n",
    "            tax = 'No Fungi'\n",
    "\n",
    "        if tax not in data_tax.keys():\n",
    "    \n",
    "            data_tax[tax] = {col: 0 for col in mearged_pike_out.columns} \n",
    "        \n",
    "        for col in mearged_pike_out.columns:\n",
    "           \n",
    "            data_tax[tax][col] += mearged_pike_out[col][av]\n",
    "    \n",
    "    data_tax_df = processing_data_tax(data_tax)\n",
    "    \n",
    "    return data_tax_df, OTU_decoder\n",
    "    \n",
    "\n",
    "def filter_data(output, \n",
    "                dbpath,\n",
    "                mearged_pike_out,\n",
    "                taxonomy_level, \n",
    "                identity_filter=95, \n",
    "                cov_lim=60, \n",
    "                evalue_filter=1e-05):\n",
    "\n",
    "    # Creating output directory\n",
    "    try:\n",
    "        \n",
    "        os.mkdir(output)\n",
    "        \n",
    "    except FileExistsError:\n",
    "        \n",
    "        print('The output directory already exists!')\n",
    "        \n",
    "    consensus = create_fasta(output, mearged_pike_out, dbpath)\n",
    "    run_blast(dbpath, output)\n",
    "    data_tax = decode_tax(dbpath)\n",
    "    blasting_results_df = parse_blast(output, \n",
    "                                      dbpath, \n",
    "                                      data_tax, \n",
    "                                      consensus, \n",
    "                                      identity_filter, \n",
    "                                      cov_lim, \n",
    "                                      evalue_filter)\n",
    "\n",
    "  #  mearged_pike_out = filter_av(mearged_pike_out, prevalence, detection, slice)\n",
    "    data_tax_df, OTU_decoder = get_taxonomy(data_tax, \n",
    "                                            blasting_results_df, \n",
    "                                            mearged_pike_out,\n",
    "                                            taxonomy_level)\n",
    "\n",
    "    data_tax_df = data_tax_df[mearged_pike_out.columns]\n",
    "    for col in data_tax_df.columns:\n",
    "        \n",
    "        data_tax_df[col] = data_tax_df[col] / np.sum(data_tax_df[col].values)\n",
    "    \n",
    "    data_tax_df = data_tax_df.fillna(0)[mearged_pike_out.columns]   \n",
    "    data_tax_df = data_tax_df.assign(m=data_tax_df.mean(axis=1)).sort_values('m').drop('m', axis=1)\n",
    "\n",
    "\n",
    "    return data_tax_df, data_tax, blasting_results_df, DataFrame(OTU_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63629b7b-c05c-4055-8d3e-5b76804c6dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_color(obj_dict):\n",
    "    \n",
    "    color = ''\n",
    "    \n",
    "    while color not in obj_dict.values() and color == '':\n",
    "        \n",
    "        color = \"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "    \n",
    "    return color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c03e83-792f-465d-a12c-a0553e8b9d2f",
   "metadata": {},
   "source": [
    "MERGING ALL TABLES INTO ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e683f226-6318-4520-8e2e-26bf9657b6b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mearged_otu_table = []\n",
    "\n",
    "for amplicon_type in ['_100000_reads', '_10000_reads', '_1000_reads', '_150000_reads', '_150_reads',\n",
    "                      '_200_reads', '_250_reads', '_300000_reads', '_30000_reads', '_3000_reads', '_300_reads',\n",
    "                      '_400000_reads', '_500000_reads', '_50000_reads', '_5000_reads', '_500_reads',\n",
    "                      '_600000_reads', '_700000_reads']:\n",
    "    \n",
    "    for sample in listdir(f'/mnt/AsusShareI2/RUNS/runs-sonec/pike_single_mode/pike_V3_V4_all/pike_V3_V4_trimmed{amplicon_type}/results/'):\n",
    "        opn_res = read_csv(f'/mnt/AsusShareI2/RUNS/runs-sonec/pike_single_mode/pike_V3_V4_all/pike_V3_V4_trimmed{amplicon_type}/results/{sample}/results.tsv', sep='\\t', index_col=0)\n",
    "        if 'Count' in opn_res.columns:\n",
    "            count = 0\n",
    "            mearged_otu_table.append(DataFrame(data=opn_res['Count'].tolist(), index=opn_res.index, columns=[sample + amplicon_type]))\n",
    "            with open(f'/mnt/AsusShareI2/RUNS/runs-sonec/pike_single_mode/pike_V3_V4_all/pike_V3_V4_trimmed{amplicon_type}/{sample}.fasta', 'w') as opn_fasta:\n",
    "                for line in opn_res.index:\n",
    "                    opn_fasta.write(f'>{count}_{opn_res[\"Count\"][line]}\\n{line}\\n')\n",
    "                    count += 1\n",
    "        else:\n",
    "            print(f'Столбец \"Count\" отсутствует в результате для образца {sample + amplicon_type}')\n",
    "\n",
    "mearged_otu_table = pd.concat(mearged_otu_table, axis=1).fillna(0)\n",
    "mearged_otu_table = mearged_otu_table.reindex(sorted(mearged_otu_table.columns), axis=1)\n",
    "mearged_otu_table.to_csv('single_V3_V4_merged_otu_table_all_samples_all_reads.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd4768-afe9-4d8b-ac0c-ce57bb5200bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mearged_otu_table.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51adec32-35ad-4197-8c15-17b970749e47",
   "metadata": {},
   "source": [
    "**WORKING WITH SILVA AND BLAST**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967d8f9f-9aea-4e5f-ab18-d31d93354e91",
   "metadata": {},
   "source": [
    "**OTU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a2426e7-2217-457c-8b62-1fe0dd026f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output directory already exists!\n",
      "\n",
      "\n",
      "Building a new DB, current time: 08/06/2024 15:54:33\n",
      "New DB name:   /mnt/AsusShareI2/RUNS/runs-sonec/SILVA_138.1_SSURef_NR99_tax_silva.fasta\n",
      "New DB title:  /mnt/AsusShareI2/RUNS/runs-sonec/SILVA_138.1_SSURef_NR99_tax_silva.fasta\n",
      "Sequence type: Nucleotide\n",
      "Deleted existing Nucleotide BLAST database named /mnt/AsusShareI2/RUNS/runs-sonec/SILVA_138.1_SSURef_NR99_tax_silva.fasta\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Adding sequences from FASTA; added 510508 sequences in 11.6544 seconds.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "510508it [00:05, 90993.34it/s]\n",
      "100%|████████████████████████████████████████| 330/330 [00:00<00:00, 979.02it/s]\n",
      "100%|███████████████████████████████████████| 315/315 [00:00<00:00, 2432.27it/s]\n"
     ]
    }
   ],
   "source": [
    "output = f'/mnt/AsusShareI2/RUNS/runs-sonec/pike_single_mode/pike_V3_V4_all/pike_V3_V4_trimmed{amplicon_type}/TAXONOMY'\n",
    "dbpath = '/mnt/AsusShareI2/RUNS/runs-sonec/SILVA_138.1_SSURef_NR99_tax_silva.fasta'\n",
    "#taxonomy_level = 'Genus'\n",
    "#taxonomy_level = 'Species'\n",
    "taxonomy_level = 'OTU'\n",
    "\n",
    "data_tax_df, data_tax, blasting_results_df, OTU_decoder = filter_data(output, \n",
    "                                                                     dbpath,\n",
    "                                                                     mearged_otu_table,\n",
    "                                                                     taxonomy_level, \n",
    "                                                                     identity_filter=95, \n",
    "                                                                     cov_lim=60, \n",
    "                                                                     evalue_filter=1e-05)\n",
    "data_tax_df.to_csv('single_V3_V4_data_tax_df_OTU.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054ce333-9a74-4de7-aa48-4af083dd7a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tax_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d91184c9-295a-49c8-b420-a0cd9c05bfd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V3_V4_1_100000_reads</th>\n",
       "      <th>V3_V4_1_10000_reads</th>\n",
       "      <th>V3_V4_1_1000_reads</th>\n",
       "      <th>V3_V4_1_100_reads</th>\n",
       "      <th>V3_V4_1_150000_reads</th>\n",
       "      <th>V3_V4_1_150_reads</th>\n",
       "      <th>V3_V4_1_200_reads</th>\n",
       "      <th>V3_V4_1_250_reads</th>\n",
       "      <th>V3_V4_1_300000_reads</th>\n",
       "      <th>V3_V4_1_30000_reads</th>\n",
       "      <th>...</th>\n",
       "      <th>V3_V4_R3_30000_reads</th>\n",
       "      <th>V3_V4_R3_3000_reads</th>\n",
       "      <th>V3_V4_R3_300_reads</th>\n",
       "      <th>V3_V4_R3_400000_reads</th>\n",
       "      <th>V3_V4_R3_500000_reads</th>\n",
       "      <th>V3_V4_R3_50000_reads</th>\n",
       "      <th>V3_V4_R3_5000_reads</th>\n",
       "      <th>V3_V4_R3_500_reads</th>\n",
       "      <th>V3_V4_R3_600000_reads</th>\n",
       "      <th>V3_V4_R3_700000_reads</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   V3_V4_1_100000_reads  V3_V4_1_10000_reads  V3_V4_1_1000_reads  \\\n",
       "0                    11                   11                  11   \n",
       "\n",
       "   V3_V4_1_100_reads  V3_V4_1_150000_reads  V3_V4_1_150_reads  \\\n",
       "0                  1                    10                  2   \n",
       "\n",
       "   V3_V4_1_200_reads  V3_V4_1_250_reads  V3_V4_1_300000_reads  \\\n",
       "0                  4                  8                    11   \n",
       "\n",
       "   V3_V4_1_30000_reads  ...  V3_V4_R3_30000_reads  V3_V4_R3_3000_reads  \\\n",
       "0                   12  ...                    17                    8   \n",
       "\n",
       "   V3_V4_R3_300_reads  V3_V4_R3_400000_reads  V3_V4_R3_500000_reads  \\\n",
       "0                   8                     24                     25   \n",
       "\n",
       "   V3_V4_R3_50000_reads  V3_V4_R3_5000_reads  V3_V4_R3_500_reads  \\\n",
       "0                    22                    8                   8   \n",
       "\n",
       "   V3_V4_R3_600000_reads  V3_V4_R3_700000_reads  \n",
       "0                     23                     25  \n",
       "\n",
       "[1 rows x 117 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tax_no0_rows = (data_tax_df !=0).sum()\n",
    "result = pd.DataFrame(data_tax_no0_rows).transpose()\n",
    "result.head(10)\n",
    "#result.index = ['OTU_count_nonzero']\n",
    "#result.to_csv('OTU_count_nonzero.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edeae6ad-a480-4e58-8bcd-4a3cdd370229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>OTU_count_nonzero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V3_V4_1_100000_reads</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V3_V4_1_10000_reads</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V3_V4_1_1000_reads</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V3_V4_1_100_reads</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V3_V4_1_150000_reads</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>V3_V4_R3_50000_reads</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>V3_V4_R3_5000_reads</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>V3_V4_R3_500_reads</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>V3_V4_R3_600000_reads</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>V3_V4_R3_700000_reads</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    column  OTU_count_nonzero\n",
       "0     V3_V4_1_100000_reads                 11\n",
       "1      V3_V4_1_10000_reads                 11\n",
       "2       V3_V4_1_1000_reads                 11\n",
       "3        V3_V4_1_100_reads                  1\n",
       "4     V3_V4_1_150000_reads                 10\n",
       "..                     ...                ...\n",
       "112   V3_V4_R3_50000_reads                 22\n",
       "113    V3_V4_R3_5000_reads                  8\n",
       "114     V3_V4_R3_500_reads                  8\n",
       "115  V3_V4_R3_600000_reads                 23\n",
       "116  V3_V4_R3_700000_reads                 25\n",
       "\n",
       "[117 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_long = result.melt(var_name='column', value_name='OTU_count_nonzero')\n",
    "result_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec94270d-2d88-48fa-aac9-b7024b031d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>OTU_count_nonzero</th>\n",
       "      <th>sample_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V3_V4_1_100000_reads</td>\n",
       "      <td>11</td>\n",
       "      <td>V3_V4_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V3_V4_1_10000_reads</td>\n",
       "      <td>11</td>\n",
       "      <td>V3_V4_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V3_V4_1_1000_reads</td>\n",
       "      <td>11</td>\n",
       "      <td>V3_V4_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V3_V4_1_100_reads</td>\n",
       "      <td>1</td>\n",
       "      <td>V3_V4_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V3_V4_1_150000_reads</td>\n",
       "      <td>10</td>\n",
       "      <td>V3_V4_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>V3_V4_R3_50000_reads</td>\n",
       "      <td>22</td>\n",
       "      <td>V3_V4_R3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>V3_V4_R3_5000_reads</td>\n",
       "      <td>8</td>\n",
       "      <td>V3_V4_R3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>V3_V4_R3_500_reads</td>\n",
       "      <td>8</td>\n",
       "      <td>V3_V4_R3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>V3_V4_R3_600000_reads</td>\n",
       "      <td>23</td>\n",
       "      <td>V3_V4_R3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>V3_V4_R3_700000_reads</td>\n",
       "      <td>25</td>\n",
       "      <td>V3_V4_R3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    column  OTU_count_nonzero sample_name\n",
       "0     V3_V4_1_100000_reads                 11     V3_V4_1\n",
       "1      V3_V4_1_10000_reads                 11     V3_V4_1\n",
       "2       V3_V4_1_1000_reads                 11     V3_V4_1\n",
       "3        V3_V4_1_100_reads                  1     V3_V4_1\n",
       "4     V3_V4_1_150000_reads                 10     V3_V4_1\n",
       "..                     ...                ...         ...\n",
       "112   V3_V4_R3_50000_reads                 22    V3_V4_R3\n",
       "113    V3_V4_R3_5000_reads                  8    V3_V4_R3\n",
       "114     V3_V4_R3_500_reads                  8    V3_V4_R3\n",
       "115  V3_V4_R3_600000_reads                 23    V3_V4_R3\n",
       "116  V3_V4_R3_700000_reads                 25    V3_V4_R3\n",
       "\n",
       "[117 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_long['sample_name'] = result_long['column'].str.extract(r'(V3_V4_(?:R)?\\d+)_\\d+_reads')\n",
    "result_long\n",
    "#result_long.to_csv('result_long_test.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e359554d-bf53-4bfd-a5fb-835320a001f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OTU_count_nonzero</th>\n",
       "      <th>sample_name</th>\n",
       "      <th>number_of_reads</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>V3_V4_1</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>V3_V4_1</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>V3_V4_1</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>V3_V4_1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>V3_V4_1</td>\n",
       "      <td>150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>22</td>\n",
       "      <td>V3_V4_R3</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>8</td>\n",
       "      <td>V3_V4_R3</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>8</td>\n",
       "      <td>V3_V4_R3</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>23</td>\n",
       "      <td>V3_V4_R3</td>\n",
       "      <td>600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>25</td>\n",
       "      <td>V3_V4_R3</td>\n",
       "      <td>700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     OTU_count_nonzero sample_name  number_of_reads\n",
       "0                   11     V3_V4_1           100000\n",
       "1                   11     V3_V4_1            10000\n",
       "2                   11     V3_V4_1             1000\n",
       "3                    1     V3_V4_1              100\n",
       "4                   10     V3_V4_1           150000\n",
       "..                 ...         ...              ...\n",
       "112                 22    V3_V4_R3            50000\n",
       "113                  8    V3_V4_R3             5000\n",
       "114                  8    V3_V4_R3              500\n",
       "115                 23    V3_V4_R3           600000\n",
       "116                 25    V3_V4_R3           700000\n",
       "\n",
       "[117 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_long['number_of_reads'] = result_long['column'].str.extract(r'V3_V4_(?:R)?\\d+_(\\d+)_reads')\n",
    "result_long.drop('column', axis=1, inplace=True)\n",
    "result_long.fillna(0)\n",
    "result_long['number_of_reads'] = result_long['number_of_reads'].astype('int')\n",
    "result_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "189706f3-ff41-4076-b7ed-017b494242ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_long=result_long.sort_values('number_of_reads',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b75e3625-1d24-47f7-a026-df29090fed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_long\n",
    "result_long.to_csv('V3_V4_OTU_count_long_table.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe15d1d-56c8-4a35-80ab-63855eca99b5",
   "metadata": {},
   "source": [
    "**PLOTTING FROM RESULT_LONG DF( OPTIMAL)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800c801b-27f7-4136-81b5-21b603405d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for sample in result_long['sample_name'].unique():\n",
    "    subset = result_long[result_long['sample_name'] == sample]\n",
    "    ax.plot(subset['number_of_reads'], subset['OTU_count_nonzero'], marker='o', linestyle='-', label=sample)\n",
    "\n",
    "ax.set_xlabel('Number of reads')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('OTU')\n",
    "ax.set_ylim(-1, 35)\n",
    "ax.set_xscale('log')\n",
    "#ax.xaxis.set_ticks(result_melted[\"number_of_reads\"])\n",
    "ax.legend()\n",
    "plt.show()\n",
    "#plt.savefig('OTU_V3_V4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ab5fd-eb09-4d04-a32f-f1b1da74c5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.Series([col.rsplit('_', 2)[0] for col in result.columns]).drop_duplicates().tolist()\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d597a47e-ad59-4cda-9e76-5f931456dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "reads = pd.Series([col.split('_')[-2] for col in result.columns]).drop_duplicates().tolist()\n",
    "reads.sort()\n",
    "reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46037c06-0247-480b-b803-79ffe57c7061",
   "metadata": {},
   "source": [
    "TESTING RESHAPING DF ON DUMMY DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051c9bdf-a786-4ad9-8636-fc5bb17d82ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your initial dataframe is named df\n",
    "data = {\n",
    "    'V3_V4_1_100_reads': [10],\n",
    "    'V3_V4_2_100_reads': [15],\n",
    "    'V3_V4_3_100_reads': [20],\n",
    "    'V3_V4_1_500_reads': [30],\n",
    "    'V3_V4_2_500_reads': [30],\n",
    "    'V3_V4_3_500_reads': [35]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Melt the dataframe to go from wide to long format\n",
    "df_long = df.melt(var_name='column', value_name='OTU_count_nonzero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0596b50d-22bf-419b-bb6b-88cc22215e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cc5854-be7a-41c2-aa94-3ed73594c7c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8168a4a1-c375-4fb8-bddb-5857de263928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sample names and number of reads\n",
    "df_long['sample_name'] = df_long['column'].str.extract(r'(V3_V4_\\d+)_\\d+_reads')\n",
    "df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34743d85-86a6-4517-ba7a-a9f3994af4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long['number_of_reads'] = df_long['column'].str.extract(r'V3_V4_\\d+_(\\d+)_reads')\n",
    "df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81b98e4-b3b8-4b64-8992-68f7ab1d55ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original column names as they are no longer needed\n",
    "df_long.drop('column', axis=1, inplace=True)\n",
    "df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e37c23-e272-4d5a-97e3-10c9a7f1ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the table to get the desired format\n",
    "df_pivot = df_long.pivot(index='number_of_reads', columns='sample_name', values='OTU_count_nonzero')\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aedfa68-531d-4668-96e5-001d5bdbee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index to get number_of_reads as a column\n",
    "df_final = df_pivot.reset_index()\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870cfa39-5071-4e65-9a96-528819b6ec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, sort by number_of_reads if needed\n",
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bc159a-cf66-43ec-9f48-155f470541a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns.name = None\n",
    "# Print the final dataframe\n",
    "print(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e0dde-fd1b-465f-9a26-fac9aeb79fa4",
   "metadata": {},
   "source": [
    "**GENUS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf04d27d-adbe-4ae7-b1d6-f11d8691a753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output directory already exists!\n",
      "\n",
      "\n",
      "Building a new DB, current time: 05/16/2024 13:17:15\n",
      "New DB name:   /mnt/AsusShareI2/RUNS/runs-sonec/SILVA_138.1_SSURef_NR99_tax_silva.fasta\n",
      "New DB title:  /mnt/AsusShareI2/RUNS/runs-sonec/SILVA_138.1_SSURef_NR99_tax_silva.fasta\n",
      "Sequence type: Nucleotide\n",
      "Deleted existing Nucleotide BLAST database named /mnt/AsusShareI2/RUNS/runs-sonec/SILVA_138.1_SSURef_NR99_tax_silva.fasta\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Adding sequences from FASTA; added 510508 sequences in 12.177 seconds.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "510508it [00:07, 71395.23it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 340/340 [00:00<00:00, 847.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 325/325 [00:00<00:00, 1968.58it/s]\n"
     ]
    }
   ],
   "source": [
    "output = f'/mnt/AsusShareI2/RUNS/runs-sonec/pike_all/pike_V3_V4_all/pike_V3_V4_trimmed{amplicon_type}/TAXONOMY'\n",
    "dbpath = '/mnt/AsusShareI2/RUNS/runs-sonec/SILVA_138.1_SSURef_NR99_tax_silva.fasta'\n",
    "taxonomy_level = 'Genus'\n",
    "#taxonomy_level = 'Species'\n",
    "#taxonomy_level = 'OTU'\n",
    "\n",
    "data_tax_df, data_tax, blasting_results_df, OTU_decoder = filter_data(output, \n",
    "                                                                     dbpath,\n",
    "                                                                     mearged_otu_table,\n",
    "                                                                     taxonomy_level, \n",
    "                                                                     identity_filter=95, \n",
    "                                                                     cov_lim=60, \n",
    "                                                                     evalue_filter=1e-05)\n",
    "data_tax_df.to_csv('V3_V4_data_tax_df_genus.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4aa2c709-0d84-4c39-81bd-7bfbe98db907",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tax_no0_rows = (data_tax_df !=0).sum()\n",
    "result = pd.DataFrame(data_tax_no0_rows).transpose()\n",
    "result.index = ['Genus_count_nonzero']\n",
    "#result.to_csv('genus_count_nonzero.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e10d8f50-e210-42db-91f6-a25da1533648",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_long = result.melt(var_name='column', value_name='Genus_count_nonzero')\n",
    "result_long['sample_name'] = result_long['column'].str.extract(r'(V3_V4_(?:R)?\\d+)_\\d+_reads')\n",
    "result_long['number_of_reads'] = result_long['column'].str.extract(r'V3_V4_(?:R)?\\d+_(\\d+)_reads')\n",
    "result_long.drop('column', axis=1, inplace=True)\n",
    "result_long.fillna(0)\n",
    "result_long['number_of_reads'] = result_long['number_of_reads'].astype('int')\n",
    "result_long=result_long.sort_values('number_of_reads',ascending=True)\n",
    "result_long\n",
    "result_long.to_csv('V3_V4_genus_count_long_table.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5777466-5172-4046-9d3f-4324409c5e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for sample in result_long['sample_name'].unique():\n",
    "    subset = result_long[result_long['sample_name'] == sample]\n",
    "    ax.plot(subset['number_of_reads'], subset['Genus_count_nonzero'], marker='o', linestyle='-', label=sample)\n",
    "\n",
    "ax.set_xlabel('Number of reads')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('GENUS')\n",
    "ax.set_ylim(-1, 35)\n",
    "ax.set_xscale('log')\n",
    "#ax.xaxis.set_ticks(result_melted[\"number_of_reads\"])\n",
    "ax.legend()\n",
    "plt.show()\n",
    "#plt.savefig('Genus_V3_V4.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9948eda7-e371-430a-8e4b-b5e0d481d734",
   "metadata": {},
   "source": [
    "SPECIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc9d9418-04aa-4aea-a6e1-5e26489b3a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output directory already exists!\n",
      "\n",
      "\n",
      "Building a new DB, current time: 05/16/2024 13:20:24\n",
      "New DB name:   /mnt/AsusShareI2/RUNS/runs-sonec/SILVA_138.1_SSURef_NR99_tax_silva.fasta\n",
      "New DB title:  /mnt/AsusShareI2/RUNS/runs-sonec/SILVA_138.1_SSURef_NR99_tax_silva.fasta\n",
      "Sequence type: Nucleotide\n",
      "Deleted existing Nucleotide BLAST database named /mnt/AsusShareI2/RUNS/runs-sonec/SILVA_138.1_SSURef_NR99_tax_silva.fasta\n",
      "Keep MBits: T\n",
      "Maximum file size: 3000000000B\n",
      "Adding sequences from FASTA; added 510508 sequences in 11.8562 seconds.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "510508it [00:07, 71995.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 340/340 [00:00<00:00, 846.61it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 325/325 [00:00<00:00, 1959.94it/s]\n"
     ]
    }
   ],
   "source": [
    "output = f'/mnt/AsusShareI2/RUNS/runs-sonec/pike_all/pike_V3_V4_all/pike_V3_V4_trimmed{amplicon_type}/TAXONOMY'\n",
    "dbpath = '/mnt/AsusShareI2/RUNS/runs-sonec/SILVA_138.1_SSURef_NR99_tax_silva.fasta'\n",
    "#taxonomy_level = 'Genus'\n",
    "taxonomy_level = 'Species'\n",
    "#taxonomy_level = 'OTU'\n",
    "\n",
    "data_tax_df, data_tax, blasting_results_df, OTU_decoder = filter_data(output, \n",
    "                                                                     dbpath,\n",
    "                                                                     mearged_otu_table,\n",
    "                                                                     taxonomy_level, \n",
    "                                                                     identity_filter=95, \n",
    "                                                                     cov_lim=60, \n",
    "                                                                     evalue_filter=1e-05)\n",
    "data_tax_df.to_csv('V3_V4_data_tax_df_species.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "748fe7e3-5068-4ee7-bd78-70121f07bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tax_no0_rows = (data_tax_df !=0).sum()\n",
    "result = pd.DataFrame(data_tax_no0_rows).transpose()\n",
    "result.index = ['Species_count_nonzero']\n",
    "#result.to_csv('species_count_nonzero.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79509fcf-eb85-46e1-8144-aaace4e42732",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_long = result.melt(var_name='column', value_name='Species_count_nonzero')\n",
    "result_long['sample_name'] = result_long['column'].str.extract(r'(V3_V4_(?:R)?\\d+)_\\d+_reads')\n",
    "result_long['number_of_reads'] = result_long['column'].str.extract(r'V3_V4_(?:R)?\\d+_(\\d+)_reads')\n",
    "result_long.drop('column', axis=1, inplace=True)\n",
    "result_long.fillna(0)\n",
    "result_long['number_of_reads'] = result_long['number_of_reads'].astype('int')\n",
    "result_long=result_long.sort_values('number_of_reads',ascending=True)\n",
    "result_long\n",
    "result_long.to_csv('V3_V4_species_count_long_table.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30faaf54-3a65-4dd8-bbe8-fa32a6087c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for sample in result_long['sample_name'].unique():\n",
    "    subset = result_long[result_long['sample_name'] == sample]\n",
    "    ax.plot(subset['number_of_reads'], subset['Species_count_nonzero'], marker='o', linestyle='-', label=sample)\n",
    "\n",
    "ax.set_xlabel('Number of reads')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Species')\n",
    "ax.set_ylim(-1, 35)\n",
    "ax.set_xscale('log')\n",
    "#ax.xaxis.set_ticks(result_melted[\"number_of_reads\"])\n",
    "ax.legend()\n",
    "plt.show()\n",
    "plt.savefig('Species_V3_V4.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e5dc73-f833-46f6-9f5d-8e59bd5ac64d",
   "metadata": {},
   "source": [
    "PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c61ee27-6888-4b94-a2c2-c5f2f1da4efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Color_collection = {}\n",
    "\n",
    "for i in data_tax_df.index:\n",
    "    \n",
    "    Color_collection[i] = get_color(Color_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4f4352-6fa9-4483-8c7d-0c7f30dc6f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "\n",
    "fig = px.bar(data_tax_df.T, \n",
    "             x=data_tax_df.columns, \n",
    "             y=data_tax_df.index,\n",
    "             width=1500, \n",
    "             height=900, \n",
    "            # color=data_tax_df.index,\n",
    "             labels={'value': 'Relative abundance', 'index':'Samples'}, \n",
    "             template='simple_white',\n",
    "             color_discrete_map=Color_collection)\n",
    "fig.update_layout(yaxis_range=[0, 1], legend_title_text='Taxon', legend_title_side='top center')\n",
    "fig.update_traces(marker_line_width=1.1, marker_line_color='#202020', opacity=0.8)\n",
    "fig.update_yaxes(ticksuffix = \"  \")\n",
    "fig.update_xaxes(range=[-1, len(mearged_otu_table.T)+0.2], autorangeoptions_clipmax=len(data_tax_df.T))\n",
    "\n",
    "#fig.update_layout(showlegend=False)\n",
    "#os.mkdir(\"VIZ\")\n",
    "fig.write_image(f\"VIZ/16S_{taxonomy_level}.pdf\")\n",
    "fig.write_image(f\"VIZ/16S_{taxonomy_level}.png\", scale=5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ba2a31-bac8-4851-aa4e-0ae3acd3d137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f05481-6c7f-40be-9c7b-56c62972ac70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ea1f28-e4a2-491d-90d3-9451823c5842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
